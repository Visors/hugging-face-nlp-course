{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-11T02:03:16.931168Z",
     "start_time": "2025-04-11T02:03:16.929127Z"
    }
   },
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:03:26.660759Z",
     "start_time": "2025-04-11T02:03:20.256585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ],
   "id": "3debe45ac0496ab0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/397 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2aafe32ed36445d48e2c6630d195219e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c37eed3febd4d61929393bc43233397"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1157cf0a603e422e9aac25aea4bd9ec2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:03:37.052393Z",
     "start_time": "2025-04-11T02:03:37.047199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ],
   "id": "4df2e5d89d4928df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'▁This': 3,\n",
       "             '▁is': 2,\n",
       "             '▁the': 1,\n",
       "             '▁Hugging': 1,\n",
       "             '▁Face': 1,\n",
       "             '▁Course.': 1,\n",
       "             '▁chapter': 1,\n",
       "             '▁about': 1,\n",
       "             '▁tokenization.': 1,\n",
       "             '▁section': 1,\n",
       "             '▁shows': 1,\n",
       "             '▁several': 1,\n",
       "             '▁tokenizer': 1,\n",
       "             '▁algorithms.': 1,\n",
       "             '▁Hopefully,': 1,\n",
       "             '▁you': 1,\n",
       "             '▁will': 1,\n",
       "             '▁be': 1,\n",
       "             '▁able': 1,\n",
       "             '▁to': 1,\n",
       "             '▁understand': 1,\n",
       "             '▁how': 1,\n",
       "             '▁they': 1,\n",
       "             '▁are': 1,\n",
       "             '▁trained': 1,\n",
       "             '▁and': 1,\n",
       "             '▁generate': 1,\n",
       "             '▁tokens.': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:03:50.558841Z",
     "start_time": "2025-04-11T02:03:50.552203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        # 循环遍历长度至少为2的子字\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq\n",
    "\n",
    "# 按频率对子词排序\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_subwords[:10]"
   ],
   "id": "710393cd43788fc9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁t', 7),\n",
       " ('is', 5),\n",
       " ('er', 5),\n",
       " ('▁a', 5),\n",
       " ('▁to', 4),\n",
       " ('to', 4),\n",
       " ('en', 4),\n",
       " ('▁T', 3),\n",
       " ('▁Th', 3),\n",
       " ('▁Thi', 3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:03:57.274820Z",
     "start_time": "2025-04-11T02:03:57.272131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token, freq in token_freqs}"
   ],
   "id": "8aa3c2a90c4cea3c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:04:21.595150Z",
     "start_time": "2025-04-11T02:04:21.592321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ],
   "id": "1772330da7ba95d5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:04:30.971078Z",
     "start_time": "2025-04-11T02:04:30.966839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    for start_idx in range(len(word)):\n",
    "        # best_score_at_start应该由循环的前面的步骤计算和填充\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                # 如果我们发现以 end_idx 结尾的更好分段,我们会更新\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # 我们没有找到单词的 tokens  -> unknown\n",
    "        return [\"<unk>\"], None\n",
    "\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score"
   ],
   "id": "c1b92b853f3057a8",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:04:37.369687Z",
     "start_time": "2025-04-11T02:04:37.367067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(encode_word(\"Hopefully\", model))\n",
    "print(encode_word(\"This\", model))"
   ],
   "id": "14bf6e7b6497f559",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n",
      "(['This'], 6.288267030694535)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:04:41.914655Z",
     "start_time": "2025-04-11T02:04:41.912102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss"
   ],
   "id": "f04fc2c29448e978",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:04:47.995283Z",
     "start_time": "2025-04-11T02:04:47.991911Z"
    }
   },
   "cell_type": "code",
   "source": "compute_loss(model)",
   "id": "a79b2c4531fe3df9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413.10377642940875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:04:54.315349Z",
     "start_time": "2025-04-11T02:04:54.312398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # 我们将保留长度为 1 的 tokens\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores"
   ],
   "id": "82f2aa53c4ef92ca",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:05:00.950660Z",
     "start_time": "2025-04-11T02:05:00.872260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scores = compute_scores(model)\n",
    "print(scores[\"ll\"])\n",
    "print(scores[\"his\"])"
   ],
   "id": "77eb7351f23d7740",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.376412403623874\n",
      "0.0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:05:07.626468Z",
     "start_time": "2025-04-11T02:05:07.192584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # 删除分数最低的percent_to_remov tokens 。\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ],
   "id": "bb7027845ec3e893",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:05:12.236151Z",
     "start_time": "2025-04-11T02:05:12.232210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "tokenize(\"This is the Hugging Face course.\", model)"
   ],
   "id": "1aab606126379a4d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁This',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁Hugging',\n",
       " '▁Face',\n",
       " '▁',\n",
       " 'c',\n",
       " 'ou',\n",
       " 'r',\n",
       " 's',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8728df9ea1936e89"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
