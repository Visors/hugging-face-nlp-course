{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-11T02:23:11.010414Z",
     "start_time": "2025-04-11T02:22:56.581092Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f58ac73f26ff47188f86c5f0e6300d61"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "163c77345efa4093b99a4b5629b0edd6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3caf6d5b8324b269c45395284c5b87e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af074775b009484ea7563df80947229a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1042db8a912f43a89d6f672cd6cebd0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a989714c3554a6c87f0a2b17cc22be5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cdb95416a744afdac7f37e37ad4d6f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:23:48.384657Z",
     "start_time": "2025-04-11T02:23:47.557292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(dataset)):\n",
    "        f.write(dataset[i][\"text\"] + \"\\n\")"
   ],
   "id": "def6a1b9d33bcc58",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:24:22.160845Z",
     "start_time": "2025-04-11T02:24:22.152257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ],
   "id": "8ba664337d1459ec",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:24:48.634548Z",
     "start_time": "2025-04-11T02:24:48.632224Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)",
   "id": "200a05d963a2f20b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:26:01.112510Z",
     "start_time": "2025-04-11T02:26:01.110151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")"
   ],
   "id": "98c86107a0cb308d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:26:07.997278Z",
     "start_time": "2025-04-11T02:26:07.994660Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))",
   "id": "c98d40611c2071b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:26:17.431662Z",
     "start_time": "2025-04-11T02:26:17.429573Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()",
   "id": "306fd8e9ada17120",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:26:26.471611Z",
     "start_time": "2025-04-11T02:26:26.469577Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()",
   "id": "5b24db48951b5250",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:26:36.530064Z",
     "start_time": "2025-04-11T02:26:36.524683Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")",
   "id": "bdee875bf5dc9474",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:26:47.672467Z",
     "start_time": "2025-04-11T02:26:47.669575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ],
   "id": "574b9316e33cd810",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Let's\", (0, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre-tokenizer.', (14, 28))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:26:56.572912Z",
     "start_time": "2025-04-11T02:26:56.569660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ],
   "id": "776fc8efde656b45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:27:04.174260Z",
     "start_time": "2025-04-11T02:27:04.171987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ],
   "id": "a8ff01c33938a030",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:27:13.636979Z",
     "start_time": "2025-04-11T02:27:09.706648Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)",
   "id": "7aab7b913b48d0ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:27:30.594019Z",
     "start_time": "2025-04-11T02:27:26.549912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
   ],
   "id": "2bfcb0f23d67cd33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:27:39.139654Z",
     "start_time": "2025-04-11T02:27:39.136712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ],
   "id": "a4aec4b3bcb8168e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:28:15.192387Z",
     "start_time": "2025-04-11T02:28:15.189722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ],
   "id": "c6aed8c0d357fe13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:28:41.277234Z",
     "start_time": "2025-04-11T02:28:41.274694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ],
   "id": "79654ce9d03b9698",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:28:49.112339Z",
     "start_time": "2025-04-11T02:28:49.109667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ],
   "id": "f98c1bb36396e3b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:28:57.753296Z",
     "start_time": "2025-04-11T02:28:57.750337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ],
   "id": "d5fe74662f6f4ade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:29:13.032026Z",
     "start_time": "2025-04-11T02:29:13.029665Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")",
   "id": "f2fb3775005aa5ed",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:29:17.652612Z",
     "start_time": "2025-04-11T02:29:17.649553Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(encoding.ids)",
   "id": "6c82535c7cd7fadf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let ' s test this tokenizer... on a pair of sentences.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:29:23.836256Z",
     "start_time": "2025-04-11T02:29:23.829762Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.save(\"tokenizer.json\")",
   "id": "600db2bd8a73c4e9",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:29:31.131278Z",
     "start_time": "2025-04-11T02:29:31.109626Z"
    }
   },
   "cell_type": "code",
   "source": "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")",
   "id": "dc3a68bc1a746af7",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:30:21.013468Z",
     "start_time": "2025-04-11T02:30:19.829608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    tokenizer_file=\"tokenizer.json\", # 也可以从tokenizer文件中加载\n",
    "    # unk_token=\"[UNK]\",\n",
    "    # pad_token=\"[PAD]\",\n",
    "    # cls_token=\"[CLS]\",\n",
    "    # sep_token=\"[SEP]\",\n",
    "    # mask_token=\"[MASK]\",\n",
    ")"
   ],
   "id": "224075e4a7ce78dd",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:30:32.715845Z",
     "start_time": "2025-04-11T02:30:32.436726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ],
   "id": "7be466e99449921a",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "796cab4a4f718a75"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
